{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Libraries Installation**"
      ],
      "metadata": {
        "id": "1iCGAhhaIiAt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bPBNekE4jp3",
        "outputId": "3eea3348-e6fa-4fbf-8f3b-ea36b6cdd2c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kafka-python in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Requirement already satisfied: confluent_kafka in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: sweetviz in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.43.0 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (4.66.6)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (1.13.1)\n",
            "Requirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (3.1.4)\n",
            "Requirement already satisfied: importlib-resources>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (6.4.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.1->sweetviz) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->sweetviz) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->sweetviz) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.3->sweetviz) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install kafka-python\n",
        "!pip install confluent_kafka\n",
        "!pip install pyspark\n",
        "!pip install sweetviz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Libraries**"
      ],
      "metadata": {
        "id": "9dBnXhuhIbjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "import threading\n",
        "import json\n",
        "\n",
        "from kafka import KafkaProducer\n",
        "from kafka.errors import KafkaError\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import sweetviz as sv\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.sql.functions import col\n",
        "from datetime import date\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import Row\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "tTXc7_x841vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Crawling and appending data to Dataframe from Yahoo finance**"
      ],
      "metadata": {
        "id": "N_PhEkygIOFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "def get_stock_data(ticker, start_date, end_date):\n",
        "    # Download historical data for the specified ticker symbol\n",
        "    data = yf.download(ticker, start=start_date, end=end_date)\n",
        "    data['Symbol'] = ticker\n",
        "    return data\n",
        "\n",
        "def get_multiple_stocks_data(tickers, start_date, end_date):\n",
        "    # Create an empty DataFrame to store the combined data\n",
        "    stock_data = pd.DataFrame()\n",
        "\n",
        "    # Loop through each ticker and fetch historical data\n",
        "    for ticker in tickers:\n",
        "        print(ticker)\n",
        "        combined_data = get_stock_data(ticker, start_date, end_date)\n",
        "        stock_data = stock_data.append(combined_data)  # Use append (deprecated, should use pd.concat instead)\n",
        "\n",
        "    return stock_data\n",
        "\n",
        "# Specify the ticker symbol and date range\n",
        "start_date = \"2023-01-01\"\n",
        "end_date = \"2024-01-01\"\n",
        "\n",
        "# Specify additional ticker symbols\n",
        "all_tickers = ['AAPL','T','GOOG','MO','AA','AXP','BABA','ABT','AMAT','AMGN','AIG','ALL','ADBE','GOOGL','ACN','ABBV', 'MT','LLY','APA','ADP','AKAM','NLY','ADSK','ADM','WBA','PANW','AMD','AVGO','EA','AEM','APD','AMBA','NVS','LULU','ARRY','A','ORLY','AZO','AN','AZN','BUD','BDX','AB','AFL','ADI','ACIW','AMP','AMTD','AEO','NVO','ALTR','PAA','AAP','FNMA','UBS','ARLP','ATI','ADT','AVB','LH','AVY','AON','ADC','AYI','ASML','AMT','ACM','ARI','AR','AAN','BAH','ALB','AIZ','SAIC','CAR','AU','APH','AMX','JKHY','AMKR','AEIS','VRSK','APO','RBA','MAA','ASX','ARCO','ANET','AIR','WAB','RS','PKG','AMG']\n",
        "\n",
        "# Get historical data for multiple stocks\n",
        "stock_data = get_multiple_stocks_data(all_tickers, start_date, end_date)\n",
        "\n",
        "# Resetting Index\n",
        "stock_data = stock_data.reset_index()\n",
        "stock_data['Date']\n"
      ],
      "metadata": {
        "id": "q1Eho3I541yP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_data.head()"
      ],
      "metadata": {
        "id": "5P9mqQcu8G2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sweetviz report for data analysis**"
      ],
      "metadata": {
        "id": "x6V9ziXmIFPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert 'Date' column to datetime type 10\n",
        "stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
        "# Group by 'Date' and 'Stock' and pivot the DataFrame\n",
        "grouped_data = stock_data.groupby(['Date', 'Symbol']).first().unstack()\n",
        "# Flatten the multi-level column index\n",
        "grouped_data.columns = ['_'.join(map(str, col)).strip() for col in grouped_data.columns.values]\n",
        "# Reset index to make 'Date' a column again\n",
        "grouped_data.reset_index(inplace=True)\n",
        "# Display the resulting DataFrame\n",
        "print(grouped_data)\n",
        "advert_report = sv.analyze(grouped_data.filter(like = 'Close'))\n",
        "advert_report.show_html('Stocks_raw_report.html')\n"
      ],
      "metadata": {
        "id": "9ROQO4Fl413o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transfering Pandas Data to pyspark DF and then to RDD**"
      ],
      "metadata": {
        "id": "lzCryr_GIuVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "spark = SparkSession.builder.appName(\"StockDataRDD\").getOrCreate()\n",
        "stock_data = stock_data.rename(columns={\"Adj Close\": \"Adj_Close\"})\n",
        "stock_data_df = spark.createDataFrame(stock_data)\n",
        "# Convert Python DataFrame to PySpark RDD\n",
        "stock_data_rdd = stock_data_df.rdd\n",
        "stock_data_df.show()\n"
      ],
      "metadata": {
        "id": "1-YvnO-W416j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Performing feature engineering using RDD map to form Daily_return and Cumulative_Return feature**\n"
      ],
      "metadata": {
        "id": "Yp8meVAMI_Mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a PySpark session\n",
        "spark = SparkSession.builder.appName(\"StockDataRDD\").getOrCreate()\n",
        "#stock_data_rdd = spark.sparkContext.parallelize(data)\n",
        "# Function to calculate returns for each stock\n",
        "def calculate_returns(partition_index, iterator):\n",
        "prev_close_price = {}\n",
        "cumulative_return = {}\n",
        "for row in iterator:\n",
        "date, open, high, low, closing_price, adj, vol, symbol = row\n",
        "# Initialize cumulative_return for the stock_symbol if not exists\n",
        "cumulative_return.setdefault(symbol, {'value': 0.0})\n",
        "# Initialize prev_close_price for the stock_symbol if not exists\n",
        "prev_close_price.setdefault(symbol, {'value': None})\n",
        "# Check if this is a new stock_symbol and reset prev_close_price\n",
        "if prev_close_price[symbol]['value'] is None:\n",
        "prev_close_price[symbol]['value'] = closing_price\n",
        "daily_return = (closing_price / prev_close_price[symbol]['value'] - 1) if (prev_close_price[symbol]['value'] is not None and closing_price is not None) else None\n",
        "cumulative_return[symbol]['value'] += daily_return if daily_return is not None else 0.0\n",
        "yield Row(\n",
        "Date=date,\n",
        "Open=open,\n",
        "High=high,\n",
        "Low=low,\n",
        "Close=closing_price,\n",
        "\n",
        "Adj_Close=adj,\n",
        "Volume=vol,\n",
        "Symbol=symbol,\n",
        "DailyReturn=daily_return,\n",
        "CumulativeReturn=cumulative_return[symbol]['value']\n",
        ")\n",
        "prev_close_price[symbol]['value'] = closing_price\n",
        "\n",
        "# Sort the RDD by Date and StockSymbol, then calculate returns using mapPartitionsWithIndex\n",
        "sorted_rdd = stock_data_rdd.sortBy(lambda x: (x[7], x[0]))\n",
        "indexed_rdd = sorted_rdd.mapPartitionsWithIndex(calculate_returns)\n",
        "# Show the resulting RDD\n",
        "indexed_rdd.collect()\n"
      ],
      "metadata": {
        "id": "Lv_--7EG419i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transfering RDD data to Pyspark dataframe**"
      ],
      "metadata": {
        "id": "-_intRtnJeRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_data_with_returns_df = indexed_rdd.toDF()\n",
        "stock_data_with_returns_df.show()"
      ],
      "metadata": {
        "id": "h-YSUa2I42AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pivot table on Date with group by on Symbols**"
      ],
      "metadata": {
        "id": "39gjOUc4JrOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df = (\n",
        "stock_data_with_returns_df.groupBy(\"Symbol\")\n",
        ".pivot(\"Date\")\n",
        ".agg(F.first(\"DailyReturn\").alias(\"DailyReturn\"), F.first(\"CumulativeReturn\").alias(\"CumulativeReturn\"))\n",
        ".orderBy(\"Symbol\")\n",
        ")\n",
        "pivot_df.show()\n"
      ],
      "metadata": {
        "id": "-ytIst3oJntJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analyzing appropriate number of clusters for applying K means**"
      ],
      "metadata": {
        "id": "MjAVxkxMJ7EL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_columns=pivot_df.columns\n",
        "feature_columns=feature_columns[1:]\n",
        "# Assemble the feature columns into a vector column\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "assembled_data = assembler.transform(pivot_df)\n",
        "# Scale the features\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
        "scaled_df = scaler.fit(assembled_data).transform(assembled_data)\n",
        "# Run k-means clustering for a range of values of k\n",
        "k_values = range(2, 11) # Adjust the range as needed\n",
        "sse = [] # Sum of squared distances\n",
        "for k in k_values:\n",
        "kmeans = KMeans(k=k, seed=1)\n",
        "model = kmeans.fit(assembled_data)\n",
        "sse.append(model.summary.trainingCost)\n",
        "# Plot the elbow curve\n",
        "plt.plot(k_values, sse, marker='o')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Sum of Squared Distances (SSE)')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Mv7te8r942DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying K-mean cluster**"
      ],
      "metadata": {
        "id": "-7HuCziPKI_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply K-Means clustering\n",
        "num_clusters = 6 # Adjust as needed\n",
        "kmeans = KMeans(k=num_clusters, seed=42, featuresCol=\"scaled_features\", predictionCol=\"cluster\")\n",
        "model = kmeans.fit(scaled_df)\n",
        "result_df = model.transform(scaled_df)\n",
        "# Display the results\n",
        "result_df.select(\"Symbol\", \"cluster\").show\n"
      ],
      "metadata": {
        "id": "Oa08H6AY42Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converting to Pandas from pyspark**"
      ],
      "metadata": {
        "id": "vlAW4JVuKY1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_df = result_df.toPandas()\n",
        "def calculate_average(lst):\n",
        "return sum(lst) / len(lst)\n",
        "pandas_df['average_column'] = pandas_df['features'].apply(lambda x: calculate_average(x))\n"
      ],
      "metadata": {
        "id": "7JQh6yLn42Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visual representation**"
      ],
      "metadata": {
        "id": "j0y-2mRZKuCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "scatter = ax.scatter(pandas_df['average_column'],pandas_df['Symbol'],c=pandas_df['cluster'],s=50)\n",
        "ax.set_title('K-Means Clustering')\n",
        "ax.set_xlabel('Avg_of_daily_and_cumulative_return')\n",
        "ax.set_ylabel('Symbol')\n",
        "plt.colorbar(scatter)\n"
      ],
      "metadata": {
        "id": "MYoTtyA742K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.catplot(data=pandas_df, x=\"cluster\", y=\"Symbol\")"
      ],
      "metadata": {
        "id": "F6ZslxPTLFky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting up Kafka**"
      ],
      "metadata": {
        "id": "Am4pFw2CLJqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -sSOL https://archive.apache.org/dist/kafka/3.3.1/kafka_2.13-3.3.1.tgz\n",
        "!tar -xzf kafka_2.13-3.3.1.tgz"
      ],
      "metadata": {
        "id": "sh4Zm3CILFnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Kafka Topics**"
      ],
      "metadata": {
        "id": "4B4--jTpLRoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./kafka_2.13-3.3.1/bin/zookeeper-server-start.sh -daemon ./kafka_2.13-3.3.1/config/zookeeper.properties\n",
        "!./kafka_2.13-3.3.1/bin/kafka-server-start.sh -daemon ./kafka_2.13-3.3.1/config/server.properties\n",
        "!echo \"Waiting for 10 secs until kafka and zookeeper services are up and running\"\n",
        "!sleep 10\n"
      ],
      "metadata": {
        "id": "EYRIsxDgLFvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Kafka Topics**"
      ],
      "metadata": {
        "id": "B-orN1qLLdnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for i in all_tickers:\n",
        "command = str('./kafka_2.13-3.3.1/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 1 --topic ') + i\n",
        "print(command)\n",
        "os.system(command)\n",
        "os.system('sleep 2')\n"
      ],
      "metadata": {
        "id": "5XlKfM08LFy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inserting Stock name and cluster number to Kafka**"
      ],
      "metadata": {
        "id": "19chEnMPLuVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = {}\n",
        "for ticker in all_tickers:\n",
        "new_data[ticker] = (pandas_df.loc[(pandas_df['Symbol'] == ticker)][['Symbol','cluster']]).to_dict()\n",
        "def error_callback(exc):\n",
        "raise Exception('Error while sendig data to kafka: {0}'.format(str(exc)))\n",
        "def write_to_kafka(topic_name, items):\n",
        "count=0\n",
        "producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'])\n",
        "for key in items:\n",
        "producer.send(topic_name, key=key.encode('utf-8')).add_errback(error_callback)\n",
        "count+=1\n",
        "producer.flush()\n",
        "print(\"Wrote {0} messages into topic: {1}\".format(count, topic_name))\n",
        "for i in all_tickers:\n",
        "write_to_kafka(i, new_data[i])\n"
      ],
      "metadata": {
        "id": "RIwUMsiBLpXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "htpQle_JLpad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Odq6i-8SLpeK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}